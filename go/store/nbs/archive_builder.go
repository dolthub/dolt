// Copyright 2024 Dolthub, Inc.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

package nbs

import (
	"context"
	"fmt"
	"io"
	"math"
	"math/rand"
	"os"
	"sort"
	"strings"
	"sync"
	"time"

	"github.com/dolthub/dolt/go/store/chunks"
	"github.com/dolthub/dolt/go/store/hash"
	"github.com/klauspost/compress/dict"
	"github.com/klauspost/compress/zstd"
	"github.com/pkg/errors"

	"github.com/valyala/gozstd"
	"golang.org/x/sync/errgroup"
)

const defaultDictionarySize = 1 << 12
const levensteinThreshold = 0.9
const dictCompressionThreshold = 0.775

type PrintfFunc func(format string, args ...interface{})

type ExpConfig struct {
	Cmp           bool // True when you want to do no compression. Used for baseline. Implies grouping = false
	Group         bool // True when you want to group chunks.
	NativeEncoder bool // True when you want to use the native encoder, false for cgo encoder
	NativeDict    bool // True when the dictionary should be generated by native go code, false for cgo. Ignored if group == false
}

func RunExperiment(ctx context.Context, cs chunks.ChunkStore, dagGroups *ChunkRelations, cfg ExpConfig, p PrintfFunc) (err error) {
	if gs, ok := cs.(*GenerationalNBS); ok {
		oldgen := gs.oldGen.tables.upstream

		for tf, ogcs := range oldgen {
			p("Processing table file: %s\n", tf.String())

			outPath := experimentOutputFile(tf)

			idx, err := ogcs.index()
			if err != nil {
				return err
			}

			if !cfg.Cmp || !cfg.Group {
				err = copyAllChunks(ctx, ogcs, idx, outPath, cfg, p)
			} else {
				err = groupAllChunks(ctx, ogcs, idx, dagGroups, outPath, cfg, p)
			}

			if err != nil {
				return err
			}

			if cfg.Cmp {
				start := time.Now()
				err = verifyAllChunks(idx, outPath)

				if err != nil {
					return err
				}
				duration := time.Since(start)

				p("Verified table file: %s\n", outPath)
				p("Verification Execution time: %v\n", duration)
			} else {
				p("Raw format. Skipping verification.\n")
			}
		}
	} else {
		return errors.New("Modern DB Expected")
	}
	return nil
}

func groupAllChunks(ctx context.Context, cs chunkSource, idx tableIndex, dagGroups *ChunkRelations, archivePath string, cfg ExpConfig, p PrintfFunc) error {
	records := make([]getRecord, idx.chunkCount())
	for i := uint32(0); i < idx.chunkCount(); i++ {
		var h hash.Hash
		_, err := idx.indexEntry(i, &h)
		if err != nil {
			return err
		}

		records = append(records, getRecord{&h, h.Prefix(), false})
	}

	file, err := os.Create(archivePath)
	if err != nil {
		return err
	}
	defer file.Close()
	writer := io.Writer(file)

	arcW := newArchiveWriter(writer)

	group, ctx := errgroup.WithContext(ctx)

	ungroupedChunks := map[hash.Hash]*chunks.Chunk{}
	groupedChunks := map[hash.Hash]*chunks.Chunk{}

	defaultSamples := make([]*chunks.Chunk, 0, idx.chunkCount()/1000)
	rando := rand.New(rand.NewSource(time.Now().UnixNano()))

	totalChunkCount := 0
	// Allocate buffer used to compress chunks.
	bottleneck := sync.Mutex{} // This code doesn't cope with parallelism yet.
	var stats Stats
	allFound, err := cs.getMany(ctx, group, records, func(_ context.Context, c *chunks.Chunk) {
		bottleneck.Lock()
		defer bottleneck.Unlock()

		totalChunkCount++

		if rando.Float64() < 0.001 {
			defaultSamples = append(defaultSamples, c)
		}

		h := c.Hash()
		if dagGroups.Contains(h) {
			groupedChunks[h] = c
		} else {
			ungroupedChunks[h] = c
		}
	}, &stats)
	if err != nil {
		panic(err)
	}
	if !allFound { // Unlikely to happen, given we got the list of chunks from this index.
		panic("not all chunks found")
	}
	err = group.Wait()
	if err != nil {
		panic(err)
	}

	var defaultDict []byte
	if len(defaultSamples) > 25 {
		defaultDict, err = buildDictionary(defaultSamples, cfg, p)
		if err != nil {
			panic(err)

		}
		p("Default Sample Size: %d\n", len(defaultSamples))
	} else {
		p("Skipping default dictionary. Not enough samples.\n")
	}

	cgList := dagGroups.convertToChunkGroups(groupedChunks, cfg, p)

	sort.Slice(cgList, func(i, j int) bool {
		return cgList[i].totalBytesSavedWDict > cgList[j].totalBytesSavedWDict
	})

	// For informational purposes.... Look at each chunk and determine if any group is a good fit. This
	// is prohibitively expensive for moderate numbers of chunks.
	/*
		for h, c := range ungroupedChunks {
			bestSimScore := 0.0
			var bestGroup *chunkGroup

			for _, cg := range cgList {
				if cg.totalBytesSavedWDict > cg.totalBytesSavedNoDict {
					simScore := similarityScore(c.Data(), cg.getLeader().Data())
					if simScore > bestSimScore {
						bestSimScore = simScore
						bestGroup = cg
					}
				}
			}

			if bestGroup != nil {
				p("You're a Brute: Adding %s to existing Group %s (sim score: %f)\n", h.String(), bestGroup.getLeader().Hash().String()[:8], bestSimScore)
				bestGroup.addChunk(c)
				delete(ungroupedChunks, h)
				groupedChunks[h] = c
				break
			}
		}
	*/
	for n, cg := range cgList {
		cg.print(n, p)
	}

	unGroupCount := 0
	groupCount := 0

	var defaultCDict *gozstd.CDict
	var defaultDictByteSpanId uint32
	if defaultDict != nil {
		defaultCDict, err = gozstd.NewCDict(defaultDict)
		if err != nil {
			return err
		}

		defaultDictByteSpanId, err = arcW.writeByteSpan(defaultDict)
		if err != nil {
			return err
		}
		if defaultDictByteSpanId != 1 {
			panic(fmt.Sprintf("Default Dict should be byte span 1. Is: %d\n", defaultDictByteSpanId))
		}
	}

	// Allocate buffer used to compress chunks.
	cmpBuff := make([]byte, 0, maxChunkSize)
	for h, c := range ungroupedChunks {
		var compressed []byte
		dictId := uint32(0)
		if cfg.NativeEncoder {
			if defaultDict != nil {
				compressed, err = zCompressDict(cmpBuff, c.Data(), defaultDict)
				dictId = defaultDictByteSpanId
			} else {
				compressed, err = zCompress(cmpBuff, c.Data())
			}
			if err != nil {
				panic(err)
			}
		} else {
			if defaultDict != nil {
				compressed = gozstd.CompressDict(cmpBuff, c.Data(), defaultCDict)
				dictId = defaultDictByteSpanId
			} else {
				compressed = gozstd.Compress(cmpBuff, c.Data())
			}
		}

		id, err := arcW.writeByteSpan(compressed)
		if err != nil {
			panic(err)
		}
		err = arcW.stageChunk(h, dictId, id)
		if err != nil {
			panic(err)
		}
		unGroupCount++
	}

	for _, cg := range cgList {
		if cg.totalBytesSavedWDict < cg.totalBytesSavedNoDict {
			// Not a good group, just write the chunks out individually.
			for _, cs := range cg.chks {
				c := cs.chunk
				if !arcW.chunkSeen(c.Hash()) {
					var compressed []byte
					dictId := uint32(0)
					if cfg.NativeEncoder {
						if defaultDict != nil {
							compressed, err = zCompressDict(cmpBuff, c.Data(), defaultDict)
							dictId = defaultDictByteSpanId
						} else {
							compressed, err = zCompress(cmpBuff, c.Data())
						}
						if err != nil {
							panic(err)
						}
					} else {
						if defaultDict != nil {
							compressed = gozstd.CompressDict(cmpBuff, c.Data(), defaultCDict)
							dictId = defaultDictByteSpanId
						} else {
							compressed = gozstd.Compress(cmpBuff, c.Data())
						}
					}

					id, err := arcW.writeByteSpan(compressed)
					if err != nil {
						panic(err)
					}
					err = arcW.stageChunk(c.Hash(), dictId, id)
					if err != nil {
						panic(err)
					}

					unGroupCount++
				} else {
					p("WARN: chunk already written: %s\n", c.Hash().String())
				}
			}
		} else {
			dictId, err := arcW.writeByteSpan(cg.dict)
			if err != nil {
				return err
			}

			for _, cs := range cg.chks {
				c := cs.chunk
				if !arcW.chunkSeen(c.Hash()) {
					var compressed []byte
					if cfg.NativeEncoder {
						compressed, err = zCompressDict(cmpBuff, c.Data(), cg.dict)
						if err != nil {
							return err
						}
					} else {
						compressed = gozstd.CompressDict(cmpBuff, c.Data(), cg.cDict)
					}

					dataId, err := arcW.writeByteSpan(compressed)
					if err != nil {
						return err
					}
					err = arcW.stageChunk(c.Hash(), dictId, dataId)
					if err != nil {
						return err
					}
					groupCount++
				} else {
					p("WARN: chunk already written: %s\n", c.Hash().String())
				}
			}
		}
	}

	n, err := arcW.writeIndex()
	if err != nil {
		return err
	}

	p("index size: %d\n", n)

	err = arcW.writeFooter(n)
	if err != nil {
		return err
	}

	err = file.Close()
	if err != nil {
		return err
	}

	p("grouped: %d\n", groupCount)
	p("ungrouped: %d\n", unGroupCount)

	if groupCount+unGroupCount != totalChunkCount {
		missing := totalChunkCount - (groupCount + unGroupCount)
		panic(fmt.Sprintf("chunk count mismatch. Missing: %d", missing))
	}

	return err
}

func experimentOutputFile(tf hash.Hash) string {
	// For the purposes of the experiment, write to the CWD.
	return fmt.Sprintf("%s.darc", tf.String())
}

// copyAllChunks copies all chunks from the given chunkSource to the given archive file. No grouping is currently done.
func copyAllChunks(ctx context.Context, cs chunkSource, idx tableIndex, archivePath string, cfg ExpConfig, p PrintfFunc) error {
	records := make([]getRecord, idx.chunkCount())
	for i := uint32(0); i < idx.chunkCount(); i++ {
		var h hash.Hash
		_, err := idx.indexEntry(i, &h)
		if err != nil {
			return err
		}

		records = append(records, getRecord{&h, h.Prefix(), false})
	}

	file, err := os.Create(archivePath)
	if err != nil {
		return err
	}
	defer file.Close()
	writer := io.Writer(file)

	arcW := newArchiveWriter(writer)

	group, ctx := errgroup.WithContext(ctx)

	// Allocate buffer used to compress chunks.
	cmpBuff := make([]byte, 0, maxChunkSize)
	var innerErr error
	bottleneck := sync.Mutex{} // This code doesn't cope with parallelism yet.
	var stats Stats
	allFound, err := cs.getMany(ctx, group, records, func(_ context.Context, c *chunks.Chunk) {
		bottleneck.Lock()
		defer bottleneck.Unlock()

		var compressed []byte
		if !cfg.Cmp {
			compressed = c.Data()
		} else if cfg.NativeEncoder {
			compressed, err = zCompress(cmpBuff, c.Data())
		} else {
			compressed = gozstd.Compress(cmpBuff, c.Data())
		}

		if err != nil {
			innerErr = err
			return
		}
		id, err := arcW.writeByteSpan(compressed)
		if err != nil {
			innerErr = err
		}
		err = arcW.stageChunk(c.Hash(), 0, id)
		if err != nil {
			innerErr = err
		}
	}, &stats)
	if err != nil {
		return err
	}
	if innerErr != nil {
		return innerErr
	}

	if !allFound { // Unlikely to happen, given we got the list of chunks from this index.
		return errors.New("not all chunks found")
	}
	err = group.Wait()
	if err != nil {
		return err
	}

	n, err := arcW.writeIndex()
	if err != nil {
		return err
	}
	return arcW.writeFooter(n)
}

func verifyAllChunks(idx tableIndex, archiveFile string) error {
	file, err := os.Open(archiveFile)
	if err != nil {
		return err
	}

	stat, err := file.Stat()
	if err != nil {
		return err
	}
	fileSize := stat.Size()

	index, err := newArchiveIndex(file, uint64(fileSize))
	if err != nil {
		return err
	}

	buff := make([]byte, 0, maxChunkSize*2)

	for i := uint32(0); i < idx.chunkCount(); i++ {
		var h hash.Hash
		_, err := idx.indexEntry(i, &h)
		if err != nil {
			return err
		}

		if !index.has(h) {
			msg := fmt.Sprintf("chunk not found in archive: %s", h.String())
			return errors.New(msg)
		}

		data, err := index.get(buff, h)
		if err != nil {
			return err
		}
		if data == nil {
			msg := fmt.Sprintf("nil data returned from archive for expected chunk: %s", h.String())
			return errors.New(msg)
		}

		chk := chunks.NewChunk(data)

		// Verify the hash of the chunk. This is the best sanity check that our data is being stored and retrieved
		// without any errors.
		if chk.Hash() != h {
			msg := fmt.Sprintf("hash mismatch for chunk: %s", h.String())
			return errors.New(msg)
		}
	}
	return nil
}

type chunkCmpScore struct {
	chunk         *chunks.Chunk
	score         float64
	dictCmpSize   int
	noDictCmpSize int
}

type chunkGroup struct {
	dict  []byte
	cDict *gozstd.CDict
	// Sorted list of chunks and their compression score. Higher is better. The score doesn't include the dictionary size.
	chks []chunkCmpScore
	// The total ratio _includes_ the dictionary size.
	totalRatioWDict       float64
	totalBytesSavedWDict  int
	totalRatioNoDict      float64
	totalBytesSavedNoDict int
	avgRawChunkSize       int
}

func newChunkGroup(cmpBuff []byte, chks []*chunks.Chunk, cfg ExpConfig, p PrintfFunc) *chunkGroup {
	scored := make([]chunkCmpScore, len(chks))
	for i, c := range chks {
		scored[i] = chunkCmpScore{c, 0.0, 0, 0}
	}
	result := chunkGroup{dict: nil, cDict: nil, chks: scored}
	result.recalculate(cmpBuff, cfg, p)
	return &result
}

func (cg *chunkGroup) getLeader() *chunks.Chunk {
	return cg.chks[0].chunk
}

// For whatever reason, we need 7 or more samples to build a dictionary. But in principle we only need 1. So duplicate
// the samples until we have enough. Note that we need to add each chunk the same number of times do we don't end up
// with bias in the dictionary.
func padSamples(chks []*chunks.Chunk) []*chunks.Chunk {
	samples := []*chunks.Chunk{}
	for len(samples) < 7 {
		for _, c := range chks {
			samples = append(samples, c)
		}
	}
	return samples
}

// Add this chunk into the group. It does not attempt to determine if this is a good addition. The caller should
// use testChunk to determine if this chunk should be added.
//
// The chunkGroup will be recalculated after this chunk is added.
func (cg *chunkGroup) addChunk(cmpBuff []byte, c *chunks.Chunk, cfg ExpConfig, p PrintfFunc) {
	scored := chunkCmpScore{
		chunk:       c,
		score:       0.0,
		dictCmpSize: 0,
	}

	cg.chks = append(cg.chks, scored)
	cg.recalculate(cmpBuff, cfg, p)
}

/*
// Remove the worst chunk from the group. The group will be recalculated after the chunk is removed. If the group
func (cg *chunkGroup) trimWorstChunk() (*chunks.Chunk, error) {
	worst := cg.chks[len(cg.chks)-1]
	cg.chks = cg.chks[:len(cg.chks)-1]
	cg.recalculate()
	return worst.chunk
}
*/

func (cg *chunkGroup) worstZScore() float64 {
	// Calculate the mean
	var sum float64
	for _, v := range cg.chks {
		sum += v.score
	}
	mean := sum / float64(len(cg.chks))

	// Calculate the sum of squares of differences from the mean
	var sumSquares float64
	for _, v := range cg.chks {
		diff := v.score - mean
		sumSquares += diff * diff
	}

	// Calculate the standard deviation
	stdDev := math.Sqrt(sumSquares / float64(len(cg.chks)))

	// Calculate z-score for the given value
	zScore := (cg.chks[len(cg.chks)-1].score - mean) / stdDev

	return zScore
}

// recalculates the group's compression ratio and re-sorts the chunks. Dictionary and total compression ratio are updated as well.
func (cg *chunkGroup) recalculate(cmpBuff []byte, cfg ExpConfig, p PrintfFunc) {
	chks := make([]*chunks.Chunk, len(cg.chks))
	for i, cs := range cg.chks {
		chks[i] = cs.chunk
	}

	samples := padSamples(chks)

	dct, err := buildDictionary(samples, cfg, p)
	if err != nil {
		panic(err)
	}

	var cDict *gozstd.CDict
	if !cfg.NativeEncoder {
		// Note that the generation of dct could be done with native of c code. Regardless, if using the C code, to encode/decode, we
		// need a cDict object.
		cDict, err = gozstd.NewCDict(dct)
		if err != nil {
			panic(err)
		}
	}

	// NM4 - we should probably avoid this alloc...
	//cmpBuff := make([]byte, 0, maxChunkSize)

	scored := make([]chunkCmpScore, len(chks))
	for i, c := range chks {

		var comp []byte
		var noDictComp []byte
		d := c.Data()
		if cfg.NativeEncoder {
			comp, err = zCompressDict(cmpBuff, d, dct)
			noDictComp, err = zCompress(cmpBuff, d)
		} else {
			comp = gozstd.CompressDict(cmpBuff, d, cDict)
			noDictComp = gozstd.Compress(cmpBuff, d)
		}

		if err != nil {
			panic(err)
		}

		scored[i] = chunkCmpScore{
			chunk:         c,
			score:         float64(len(d)-len(comp)) / float64(len(d)),
			dictCmpSize:   len(comp),
			noDictCmpSize: len(noDictComp),
		}
	}
	sort.Slice(scored, func(i, j int) bool {
		return scored[i].score > scored[j].score
	})

	cg.dict = dct
	cg.cDict = cDict
	cg.chks = scored

	raw := 0
	dictCmpSize := 0
	noDictCmpSize := 0
	for _, cs := range cg.chks {
		c := cs.chunk
		raw += len(c.Data())
		dictCmpSize += cs.dictCmpSize
		noDictCmpSize += cs.noDictCmpSize
	}
	dictCmpSize += len(dct)

	cg.totalRatioWDict = float64(raw-dictCmpSize) / float64(raw)
	cg.totalBytesSavedWDict = raw - dictCmpSize

	cg.totalRatioNoDict = float64(raw-noDictCmpSize) / float64(raw)
	cg.totalBytesSavedNoDict = raw - noDictCmpSize

	cg.avgRawChunkSize = raw / len(chks)

}

// Helper method to build new dictionary objects from a set of chunks.
func buildDictionary(chks []*chunks.Chunk, cfg ExpConfig, p PrintfFunc) (ans []byte, err error) {
	samples := [][]byte{}
	for _, c := range chks {
		samples = append(samples, c.Data())
	}

	if cfg.NativeDict {
		o := dict.Options{
			MaxDictSize:    defaultDictionarySize, // Make that a param.
			HashBytes:      6,                     // Not sure? try 4, and measure... something?? NM4.
			Output:         nil,                   // This is just for debugging
			ZstdDictID:     0,
			ZstdDictCompat: false, // This is for older version compatibility.
			ZstdLevel:      zstd.SpeedBetterCompression,
		}

		defer func() {
			if r := recover(); r != nil {
				// Write each sample to a file for debugging. NM4 - rip out this code when we've ironed out the native dict bugs.
				for i, s := range samples {
					f, err := os.Create(fmt.Sprintf("sample_%d.bin", i))
					if err != nil {
						p("Error writing sample file: %v\n", err)
						continue
					}
					_, err = f.Write(s)
					if err != nil {
						p("Error writing sample file: %v\n", err)
					}
					f.Close()
				}

				// Handle or log the panic
				ans = nil
				err = fmt.Errorf("Panic: %v", r)
			}
		}()

		ans, err = dict.BuildZstdDict(samples, o)
	} else {
		ans = gozstd.BuildDict(samples, defaultDictionarySize)
	}

	return
}

// Returns true if the chunk's compression ratio (using the existing dictionary) is better than the group's worst chunk.
func (cg *chunkGroup) testChunk(cfg ExpConfig, c *chunks.Chunk) (bool, error) {
	var comp []byte
	var err error
	if cfg.NativeEncoder {
		comp, err = zCompressDict(nil, c.Data(), cg.dict)
		if err != nil {
			return false, err
		}
	} else {
		comp = gozstd.CompressDict(nil, c.Data(), cg.cDict)
	}

	ratio := float64(len(c.Data())-len(comp)) / float64(len(c.Data()))

	if ratio > cg.chks[len(cg.chks)-1].score {
		return true, nil
	}
	return false, nil
}

func (cg *chunkGroup) print(n int, p PrintfFunc) {

	var scores []string
	for _, c := range cg.chks {
		scores = append(scores, fmt.Sprintf("%.3f", c.score))
	}

	p("-- GROUP %d -- %s -- \n", n, cg.getLeader().Hash().String()[:8])
	p("totalRatioWDict: %f (bytes saved: %d)\n", cg.totalRatioWDict, cg.totalBytesSavedWDict)
	p("totalRatioNoDict: %f (bytes saved: %d)\n", cg.totalRatioNoDict, cg.totalBytesSavedNoDict)
	p("Worst z-score: %f\n", cg.worstZScore())
	p("  Scores: %s\n", strings.Join(scores, ", "))
	p("chunks: %d (avg size: %d)\n", len(cg.chks), cg.avgRawChunkSize)
	p("Dict Size: %d\n", len(cg.dict))

}

func NewChunkRelations() ChunkRelations {
	m := make(map[hash.Hash]*hash.HashSet)
	return ChunkRelations{m}
}

type ChunkRelations struct {
	manyToGroup map[hash.Hash]*hash.HashSet
}

func (cr *ChunkRelations) Count() int {
	return len(cr.manyToGroup)
}

func (cr *ChunkRelations) convertToChunkGroups(chks map[hash.Hash]*chunks.Chunk, cfg ExpConfig, p PrintfFunc) []*chunkGroup {
	result := make([]*chunkGroup, 0, cr.Count())

	buff := make([]byte, 0, maxChunkSize)

	// For each group, look up the addresses and build a chunk group.
	for _, v := range cr.groups() {
		var c []*chunks.Chunk
		for h := range v {
			c = append(c, chks[h])
		}

		result = append(result, newChunkGroup(buff, c, cfg, p))
	}
	return result
}

func (cr *ChunkRelations) groups() []hash.HashSet {
	seen := map[*hash.HashSet]struct{}{}
	groups := make([]hash.HashSet, 0, len(cr.manyToGroup))
	for _, v := range cr.manyToGroup {
		if _, ok := seen[v]; !ok {
			groups = append(groups, *v)
			seen[v] = struct{}{}
		}
	}
	return groups
}

func (cr *ChunkRelations) Contains(h hash.Hash) bool {
	_, ok := cr.manyToGroup[h]
	return ok
}

// testAdd will take two hashes and a map of chunks, and determine if adding |chk| to the group containing |to| will
// result in a better group. This requires building a temporary chunkGroup, which is why the map of chunks is required
//
// The only error that can be returned is if the |to| hash is not in a group.
/*
func (cr *ChunkRelations) testAdd(chk *chunks.Chunk, to hash.Hash, chks map[hash.Hash]*chunks.Chunk, p PrintfFunc) (bool, hash.Hash, error) {
	if hs, ok := cr.manyToGroup[to]; ok {
		chunkList := []*chunks.Chunk{}
		for h := range *hs {
			chunkList = append(chunkList, chks[h])
		}

		cg := newChunkGroup(chunkList, p)
		return cg.testChunk(chk), cg.getLeader().Hash(), nil
	} else {
		return false, hash.Hash{}, fmt.Errorf("to hash not in group")
	}
}
*/

// Add a pair of hashes to the relations. If either chunk is already in a group, the other chunk will be added to that.
// This method has no access to the chunks themselves, so it cannot determine if the chunks are similar. This method
// is used if you know from other sources that the chunks are similar.
func (cr *ChunkRelations) Add(a, b hash.Hash) {
	aNew := true
	bNew := true
	if _, ok := cr.manyToGroup[a]; ok {
		aNew = false
	}
	if _, ok := cr.manyToGroup[b]; ok {
		bNew = false
	}

	if aNew && bNew {
		newGroup := hash.NewHashSet(a, b)

		cr.manyToGroup[a] = &newGroup
		cr.manyToGroup[b] = &newGroup
		return
	}

	if !aNew && bNew {
		cr.manyToGroup[a].Insert(b)
		cr.manyToGroup[b] = cr.manyToGroup[a]
		return
	}

	if aNew && !bNew {
		cr.manyToGroup[b].Insert(a)
		cr.manyToGroup[a] = cr.manyToGroup[b]
		return
	}

	// Both are not new, and they are already in the same group.
	if cr.manyToGroup[a] == cr.manyToGroup[b] {
		return
	}

	// Both are not new, and they are in different groups. Merge the groups.
	merged := hash.NewHashSet()
	for h := range *cr.manyToGroup[a] {
		merged.Insert(h)
	}
	for h := range *cr.manyToGroup[b] {
		merged.Insert(h)
	}
	for h := range merged {
		cr.manyToGroup[h] = &merged
	}
}

/*
func levensteinGrouping(sims []*chunks.Chunk, scoreThreshold float64) []*chunkGroup {
	type highScore struct {
		chunk *chunks.Chunk
		score float64
	}
	scoreBoard := map[*chunks.Chunk]highScore{}

	for i := 0; i < len(sims); i++ {
		for j := i + 1; j < len(sims); j++ {
			score := similarityScore(sims[i].Data(), sims[j].Data())

			if scoreBoard[sims[i]] == (highScore{}) || score > scoreBoard[sims[i]].score {
				scoreBoard[sims[i]] = highScore{sims[j], score}
			}
			if scoreBoard[sims[j]] == (highScore{}) || score > scoreBoard[sims[j]].score {
				scoreBoard[sims[j]] = highScore{sims[i], score}
			}
		}
	}

	// gather scores, then sort
	scores := map[*chunks.Chunk][]*chunks.Chunk{}
	for k, v := range scoreBoard {
		scores[v.chunk] = append(scores[v.chunk], k)
	}
	sort.Slice(sims, func(i, j int) bool {
		return len(scores[sims[i]]) > len(scores[sims[j]])
	})

	groupSeq := 1

	leaders := map[int]*chunks.Chunk{}
	groups := map[int][]*chunks.Chunk{}
	similarityGroups := map[*chunks.Chunk]int{}
	for _, c := range sims {
		// loop over scores, and ensure no member of the set is already in a group.
		leader := c
		followers := scores[c]

		// If the leader is not in a group, then we can assign a new group.
		if similarityGroups[leader] == 0 {
			leaders[groupSeq] = leader
			similarityGroups[leader] = groupSeq
			groups[groupSeq] = append(groups[groupSeq], leader)
			for _, f := range followers {
				similarityGroups[f] = groupSeq
				groups[groupSeq] = append(groups[groupSeq], f)
			}
			groupSeq++
		} else {
			for _, f := range followers {
				similarityGroups[f] = similarityGroups[leader]
				groups[similarityGroups[leader]] = append(groups[similarityGroups[leader]], f)
			}
		}
	}

	// Now see if any group leaders are close enough to join groups
	for targetGrp := 1; targetGrp < groupSeq; targetGrp++ {
		if leader, ok := leaders[targetGrp]; ok {
			for merge := targetGrp + 1; merge < groupSeq; merge++ {
				if otherLeader, ok := leaders[merge]; ok {
					if similarityScore(leader.Data(), otherLeader.Data()) > scoreThreshold {
						// merge the groups
						for _, c := range groups[merge] {
							similarityGroups[c] = targetGrp
							groups[targetGrp] = append(groups[targetGrp], c)
						}
						delete(groups, merge)
						delete(leaders, merge)
					}
				}
			}
		}
	}

	var result []*chunkGroup
	for i := 1; i < groupSeq; i++ {
		if leader, ok := leaders[i]; ok {
			if len(groups[i]) >= 7 {
				cg := buildChunkGroup(leader, groups[i])
				result = append(result, cg)
			}
		}
	}
	return result
}
*/

// Given two byte slices, return a similarity score between 0 and 1. 1 means the slices are identical, 0 means they are
// completely different.
func similarityScore(a, b []byte) float64 {

	maxLen := max(len(a), len(b))

	lev := levenshteinDistance(a, b)

	levScore := float64(maxLen-lev) / float64(maxLen)
	return levScore
}

func levenshteinDistance(a, b []byte) int {
	m, n := len(a), len(b)
	if m == 0 {
		return n
	}
	if n == 0 {
		return m
	}

	lev := 0

	if m == n {
		// If the lengths are the same, we can just compare the bytes. Saves allocation, and turns out to be pretty common.
		for i := 0; i < m; i++ {
			if a[i] != b[i] {
				lev++
			}
		}
	} else {
		matrix := make([][]int, m+1)
		for i := range matrix {
			matrix[i] = make([]int, n+1)
			matrix[i][0] = i
		}
		for j := 0; j <= n; j++ {
			matrix[0][j] = j
		}

		for i := 1; i <= m; i++ {
			for j := 1; j <= n; j++ {
				cost := 0
				if a[i-1] != b[j-1] {
					cost = 1
				}
				matrix[i][j] = min(matrix[i-1][j]+1, matrix[i][j-1]+1, matrix[i-1][j-1]+cost)
			}
		}

		lev = matrix[m][n]
	}

	return lev
}

// Compress input to output.
func zCompress(dst, data []byte) ([]byte, error) {
	//	opt1 := zstd.WithEncoderLevel(zstd.SpeedBestCompression) // SOOO SLOW.

	enc, err := zstd.NewWriter(nil)
	if err != nil {
		return nil, err
	}
	defer enc.Close()

	result := enc.EncodeAll(data, dst[:0])
	return result, nil
}

func zCompressDict(dst, data, dct []byte) ([]byte, error) {
	encoder, err := zstd.NewWriter(nil, zstd.WithEncoderDict(dct))
	if err != nil {
		return nil, err
	}
	defer encoder.Close()

	result := encoder.EncodeAll(data, dst[:0])
	return result, nil
}
